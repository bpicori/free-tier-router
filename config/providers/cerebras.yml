# Cerebras Provider Configuration
#
# API Docs: https://inference-docs.cerebras.ai/
# Cerebras offers extremely fast inference due to custom silicon.

name: cerebras
display_name: Cerebras
base_url: https://api.cerebras.ai/v1

# Default rate limits
defaults:
  limits:
    requests_per_minute: 30
    requests_per_hour: 900
    requests_per_day: 14400
    tokens_per_minute: 60000
    tokens_per_day: 1000000

# Model mappings: canonical ID -> Cerebras's model ID
# Cerebras uses clean model names without suffixes
models:
  # Tier 3 - Large Models
  - canonical: llama-3.3-70b
    id: llama-3.3-70b

  - canonical: llama-3.1-70b
    id: llama-3.1-70b

  - canonical: qwen-2.5-72b
    id: qwen-2.5-72b

  # Tier 2 - Medium Models
  - canonical: qwen-2.5-32b
    id: qwen-2.5-32b

  # Tier 1 - Small Models
  - canonical: llama-3.2-3b
    id: llama-3.2-3b

  - canonical: llama-3.2-1b
    id: llama-3.2-1b

  - canonical: llama-3.1-8b
    id: llama-3.1-8b
